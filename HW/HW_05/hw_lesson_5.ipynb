{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12842f",
   "metadata": {},
   "source": [
    "## Задание 1. Написать теггер на данных с русским языком"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd15a1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pyconll\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tag import DefaultTagger,UnigramTagger,BigramTagger,TrigramTagger\n",
    "from nltk.tag import RegexpTagger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66ed4740",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pyconll.load_from_file('.\\\\datasets\\\\ru_syntagrus-ud-train-full.conllu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a57a8c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pyconll.load_from_file('.\\\\datasets\\\\ru_syntagrus-ud-test.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3c4e83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Анкета NOUN\n",
      ". PUNCT\n",
      "\n",
      "Начальник NOUN\n",
      "областного ADJ\n",
      "управления NOUN\n",
      "связи NOUN\n",
      "Семен PROPN\n",
      "Еремеевич PROPN\n",
      "был AUX\n",
      "человек NOUN\n",
      "простой ADJ\n",
      ", PUNCT\n",
      "приходил VERB\n",
      "на ADP\n",
      "работу NOUN\n",
      "всегда ADV\n",
      "вовремя ADV\n",
      ", PUNCT\n",
      "здоровался VERB\n",
      "с ADP\n",
      "секретаршей NOUN\n",
      "за ADP\n",
      "руку NOUN\n",
      "и CCONJ\n",
      "иногда ADV\n",
      "даже PART\n",
      "писал VERB\n",
      "в ADP\n",
      "стенгазету NOUN\n",
      "заметки NOUN\n",
      "под ADP\n",
      "псевдонимом NOUN\n",
      "\" PUNCT\n",
      "Муха NOUN\n",
      "\" PUNCT\n",
      ". PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in train[:2]:\n",
    "    for token in sentence:\n",
    "        print(token.form, token.upos)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad3992bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_test_data(fulldata):\n",
    "    res = []\n",
    "    for sentence in fulldata:\n",
    "        sub_res = []\n",
    "        for token in sentence:\n",
    "            sub_res.append((token.form, token.upos))\n",
    "        res.append(sub_res)\n",
    "    return res\n",
    "\n",
    "def convert_to_test_sent(fulldata):\n",
    "    res = []\n",
    "    for sentence in fulldata:\n",
    "        sub_res = []\n",
    "        for token in sentence:\n",
    "            sub_res.append(token.form)\n",
    "        res.append(sub_res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c159341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = convert_to_test_data(train)\n",
    "test_data = convert_to_test_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42998e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb5d853f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Анкета', 'NOUN'), ('.', 'PUNCT')], [('Начальник', 'NOUN'), ('областного', 'ADJ'), ('управления', 'NOUN'), ('связи', 'NOUN'), ('Семен', 'PROPN'), ('Еремеевич', 'PROPN'), ('был', 'AUX'), ('человек', 'NOUN'), ('простой', 'ADJ'), (',', 'PUNCT'), ('приходил', 'VERB'), ('на', 'ADP'), ('работу', 'NOUN'), ('всегда', 'ADV'), ('вовремя', 'ADV'), (',', 'PUNCT'), ('здоровался', 'VERB'), ('с', 'ADP'), ('секретаршей', 'NOUN'), ('за', 'ADP'), ('руку', 'NOUN'), ('и', 'CCONJ'), ('иногда', 'ADV'), ('даже', 'PART'), ('писал', 'VERB'), ('в', 'ADP'), ('стенгазету', 'NOUN'), ('заметки', 'NOUN'), ('под', 'ADP'), ('псевдонимом', 'NOUN'), ('\"', 'PUNCT'), ('Муха', 'NOUN'), ('\"', 'PUNCT'), ('.', 'PUNCT')], [('В', 'ADP'), ('приемной', 'NOUN'), ('его', 'PRON'), ('с', 'ADP'), ('утра', 'NOUN'), ('ожидали', 'VERB'), ('посетители', 'NOUN'), (',', 'PUNCT'), ('-', 'PUNCT'), ('кое-кто', 'PRON'), ('с', 'ADP'), ('важными', 'ADJ'), ('делами', 'NOUN'), (',', 'PUNCT'), ('а', 'CCONJ'), ('кое-кто', 'PRON'), ('и', 'PART'), ('с', 'ADP'), ('такими', 'DET'), (',', 'PUNCT'), ('которые', 'PRON'), ('легко', 'ADV'), ('можно', 'ADV'), ('было', 'AUX'), ('решить', 'VERB'), ('в', 'ADP'), ('нижестоящих', 'ADJ'), ('инстанциях', 'NOUN'), (',', 'PUNCT'), ('не', 'PART'), ('затрудняя', 'VERB'), ('Семена', 'PROPN'), ('Еремеевича', 'PROPN'), ('.', 'PUNCT')], [('Однако', 'ADV'), ('стиль', 'NOUN'), ('работы', 'NOUN'), ('Семена', 'PROPN'), ('Еремеевича', 'PROPN'), ('заключался', 'VERB'), ('в', 'ADP'), ('том', 'PRON'), (',', 'PUNCT'), ('чтобы', 'SCONJ'), ('принимать', 'VERB'), ('всех', 'DET'), ('желающих', 'VERB'), ('и', 'CCONJ'), ('лично', 'ADV'), ('вникать', 'VERB'), ('в', 'ADP'), ('дело', 'NOUN'), ('.', 'PUNCT')], [('Приемная', 'NOUN'), ('была', 'AUX'), ('обставлена', 'VERB'), ('просто', 'ADV'), (',', 'PUNCT'), ('но', 'CCONJ'), ('по-деловому', 'ADV'), ('.', 'PUNCT')]]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63e14b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('В', 'ADP'), ('советский', 'ADJ'), ('период', 'NOUN'), ('времени', 'NOUN'), ('число', 'NOUN'), ('ИТ', 'PROPN'), ('-', 'PUNCT'), ('специалистов', 'NOUN'), ('в', 'ADP'), ('Армении', 'PROPN'), ('составляло', 'VERB'), ('около', 'ADP'), ('десяти', 'NUM'), ('тысяч', 'NOUN'), ('.', 'PUNCT')], [('Доставшийся', 'VERB'), ('в', 'ADP'), ('наследство', 'NOUN'), ('от', 'ADP'), ('советского', 'ADJ'), ('периода', 'NOUN'), ('времени', 'NOUN'), ('промышленный', 'ADJ'), ('и', 'CCONJ'), ('интеллектуальный', 'ADJ'), ('потенциал', 'NOUN'), ('оказался', 'VERB'), ('благом', 'NOUN'), ('и', 'CCONJ'), ('горем', 'NOUN'), ('страны', 'NOUN'), ('.', 'PUNCT')], [('С', 'ADP'), ('одной', 'NUM'), ('стороны', 'NOUN'), (',', 'PUNCT'), ('квалифицированные', 'VERB'), ('кадры', 'NOUN'), ('и', 'CCONJ'), ('развитая', 'VERB'), ('производственная', 'ADJ'), ('инфраструктура', 'NOUN'), ('резко', 'ADV'), ('отличали', 'VERB'), ('Армению', 'PROPN'), ('от', 'ADP'), ('других', 'ADJ'), ('регионов', 'NOUN'), ('СССР', 'PROPN'), (',', 'PUNCT'), ('где', 'ADV'), ('доминировали', 'VERB'), ('добывающие', 'VERB'), ('отрасли', 'NOUN'), (',', 'PUNCT'), ('а', 'CCONJ'), ('экономика', 'NOUN'), ('строилась', 'VERB'), ('на', 'ADP'), ('поставке', 'NOUN'), ('сырьевых', 'ADJ'), ('ресурсов', 'NOUN'), ('.', 'PUNCT')], [('С', 'ADP'), ('другой', 'ADJ'), (',', 'PUNCT'), ('оставшись', 'VERB'), ('без', 'ADP'), ('сырья', 'NOUN'), ('для', 'ADP'), ('промышленных', 'ADJ'), ('предприятий', 'NOUN'), (',', 'PUNCT'), ('энергоресурсов', 'NOUN'), ('и', 'CCONJ'), ('рынков', 'NOUN'), ('сбыта', 'NOUN'), ('продукции', 'NOUN'), (',', 'PUNCT'), ('конкурентоспособной', 'ADJ'), ('только', 'PART'), ('на', 'ADP'), ('постсоветском', 'ADJ'), ('пространстве', 'NOUN'), (',', 'PUNCT'), ('Армения', 'PROPN'), ('быстро', 'ADV'), ('потеряла', 'VERB'), ('темпы', 'NOUN'), ('своего', 'DET'), ('экономического', 'ADJ'), ('развития', 'NOUN'), ('.', 'PUNCT')], [('Помощь', 'NOUN'), ('этой', 'DET'), ('стране', 'NOUN'), ('обычно', 'ADV'), ('поступает', 'VERB'), ('извне', 'ADV'), ('.', 'PUNCT')]]\n"
     ]
    }
   ],
   "source": [
    "print(test_data[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfc18ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['В', 'советский', 'период', 'времени', 'число', 'ИТ', '-', 'специалистов', 'в', 'Армении', 'составляло', 'около', 'десяти', 'тысяч', '.']\n"
     ]
    }
   ],
   "source": [
    "test_sent = convert_to_test_sent(test)[0]\n",
    "print(test_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba2a1d7",
   "metadata": {},
   "source": [
    "#### 1.Проверить UnigramTagger, BigramTagger, TrigramTagger и их комбинации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "518a9dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\Python37\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('В', 'ADP'),\n",
       " ('советский', 'ADJ'),\n",
       " ('период', 'NOUN'),\n",
       " ('времени', 'NOUN'),\n",
       " ('число', 'NOUN'),\n",
       " ('ИТ', 'PROPN'),\n",
       " ('-', 'PUNCT'),\n",
       " ('специалистов', 'NOUN'),\n",
       " ('в', 'ADP'),\n",
       " ('Армении', 'PROPN'),\n",
       " ('составляло', 'VERB'),\n",
       " ('около', 'ADP'),\n",
       " ('десяти', 'NUM'),\n",
       " ('тысяч', 'NOUN'),\n",
       " ('.', 'PUNCT')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8880238497616922"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unigram_tagger = UnigramTagger(train_data)\n",
    "display(unigram_tagger.tag(test_sent), unigram_tagger.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64504f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\Python37\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('В', 'ADP'),\n",
       " ('советский', 'ADJ'),\n",
       " ('период', 'NOUN'),\n",
       " ('времени', 'NOUN'),\n",
       " ('число', 'NOUN'),\n",
       " ('ИТ', 'PROPN'),\n",
       " ('-', 'PUNCT'),\n",
       " ('специалистов', 'NOUN'),\n",
       " ('в', 'ADP'),\n",
       " ('Армении', 'PROPN'),\n",
       " ('составляло', 'VERB'),\n",
       " ('около', 'ADP'),\n",
       " ('десяти', 'NUM'),\n",
       " ('тысяч', 'NOUN'),\n",
       " ('.', 'PUNCT')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.89297989100507"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bigram_tagger = BigramTagger(train_data, backoff=unigram_tagger)\n",
    "display(bigram_tagger.tag(test_sent), bigram_tagger.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bb237e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\Python37\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('В', 'ADP'),\n",
       " ('советский', 'ADJ'),\n",
       " ('период', 'NOUN'),\n",
       " ('времени', 'NOUN'),\n",
       " ('число', 'NOUN'),\n",
       " ('ИТ', 'NOUN'),\n",
       " ('-', 'PUNCT'),\n",
       " ('специалистов', 'NOUN'),\n",
       " ('в', 'ADP'),\n",
       " ('Армении', 'PROPN'),\n",
       " ('составляло', 'VERB'),\n",
       " ('около', 'ADP'),\n",
       " ('десяти', 'NUM'),\n",
       " ('тысяч', 'NOUN'),\n",
       " ('.', 'PUNCT')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8918848780611308"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trigram_tagger = TrigramTagger(train_data, backoff=bigram_tagger)\n",
    "display(trigram_tagger.tag(test_sent), trigram_tagger.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "341f2f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Комбо тэггеров\n",
    "def backoff_tagger(train_sents, tagger_classes, backoff=None):\n",
    "    for cls in tagger_classes:\n",
    "        backoff = cls(train_sents, backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0525a0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\Python37\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8914861161220085"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backoff = DefaultTagger('NN') \n",
    "tag = backoff_tagger(train_data, [UnigramTagger, BigramTagger, TrigramTagger], backoff = backoff) \n",
    "  \n",
    "tag.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a558a8",
   "metadata": {},
   "source": [
    "#### 2.Написать свой теггер как на занятии, но улучшить попробовать разные векторайзеры, добавить знание не только букв и слов но и совместно объединить эти признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ca3539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c119176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyconll.unit.conll.Conll at 0x1e2c7e13ec8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ac33854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyconll.unit.conll.Conll at 0x1e3717810c8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18e2d764",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata_train = []\n",
    "\n",
    "for sentence in train[:]:\n",
    "    for token in sentence:\n",
    "        if token.form != None:\n",
    "            fdata_train.append([(token.form.lower(), token.upos)])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f95131fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata_test = []\n",
    "\n",
    "for sentence in test[:]:\n",
    "    for token in sentence:\n",
    "        if token.form != None:\n",
    "            fdata_test.append([(token.form.lower(), token.upos)])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "309ea3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata_sent_train = []\n",
    "for sentence in train[:]:\n",
    "    for token in sentence:\n",
    "        if token.form != None:\n",
    "            fdata_sent_train.append([(token.form.lower(), token.upos)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb207015",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata_sent_test = []\n",
    "\n",
    "for sentence in test[:]:\n",
    "    for token in sentence:\n",
    "        if token.form != None:\n",
    "            fdata_sent_test.append([(token.form.lower(), token.upos)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afa31fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наибольшая длина предложения 205\n",
      "Наибольшая длина токена 47\n"
     ]
    }
   ],
   "source": [
    "lenght_sentence = []\n",
    "lenght_token = []\n",
    "\n",
    "for sentence in train:\n",
    "    lenght_sentence.append(len(sentence))\n",
    "\n",
    "MAX_SENT_LEN = max(lenght_sentence)\n",
    "\n",
    "for sentence in train:\n",
    "    for token in sentence:\n",
    "        if token.form != None:\n",
    "            lenght_token.append(len(token.form))\n",
    "\n",
    "MAX_ORIG_TOKEN_LEN = max(lenght_token)\n",
    "\n",
    "print('Наибольшая длина предложения', MAX_SENT_LEN)\n",
    "print('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c01730c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN',\n",
       "       'NO_TAG', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM',\n",
       "       'VERB', 'X'], dtype='<U6')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_token = []\n",
    "train_label = []\n",
    "for sentence in fdata_train[:]:\n",
    "    for token in sentence:\n",
    "        train_token.append(token[0])\n",
    "        train_label.append('NO_TAG' if token[1] is None else token[1])\n",
    "        \n",
    "test_token = []\n",
    "test_label = []\n",
    "for sentence in fdata_test[:]:\n",
    "    for token in sentence:\n",
    "        test_token.append(token[0])\n",
    "        test_label.append('NO_TAG' if token[1] is None else token[1])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_enc_labels = label_encoder.fit_transform(train_label) \n",
    "test_enc_labels = label_encoder.transform(test_label)\n",
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed81545f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерность данных Train = (1205684, 169060)\n",
      "Размерность данных Test = (157912, 169060)\n",
      " Точность для векторайзера <class 'sklearn.feature_extraction.text.CountVectorizer'> = 0.9411571001570495\n",
      "Размерность данных Train = (1205684, 1048576)\n",
      "Размерность данных Test = (157912, 1048576)\n",
      " Точность для векторайзера <class 'sklearn.feature_extraction.text.HashingVectorizer'> = 0.9447920360707229\n",
      "Размерность данных Train = (1205684, 169060)\n",
      "Размерность данных Test = (157912, 169060)\n",
      " Точность для векторайзера <class 'sklearn.feature_extraction.text.TfidfVectorizer'> = 0.9459382440853134\n",
      "Wall time: 21min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Aнaлизируем по буквам векторайзеры\n",
    "\n",
    "for vectorizer in [CountVectorizer, HashingVectorizer, TfidfVectorizer]:\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "    coder = vectorizer(ngram_range=(1, 5), analyzer='char')\n",
    "    \n",
    "    X_train = coder.fit_transform(train_token)\n",
    "    X_test = coder.transform(test_token)\n",
    "    \n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)    \n",
    "    \n",
    "    print(f'Размерность данных Train = {X_train.shape}')\n",
    "    print(f'Размерность данных Test = {X_test.shape}')\n",
    "    logistic_reg = LogisticRegression(random_state=42, max_iter=100, n_jobs=4)\n",
    "    logistic_reg.fit(X_train, train_enc_labels)\n",
    "\n",
    "    pred = logistic_reg.predict(X_test)\n",
    "\n",
    "    print(f' Точность для векторайзера {vectorizer} = {accuracy_score(test_enc_labels, pred)}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9cbc25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерность данных Train = (1205684, 121903)\n",
      "Размерность данных Test = (157912, 121903)\n",
      " Точность для векторайзера <class 'sklearn.feature_extraction.text.CountVectorizer'> = 0.759403971832413\n",
      "Размерность данных Train = (1205684, 1048576)\n",
      "Размерность данных Test = (157912, 1048576)\n",
      " Точность для векторайзера <class 'sklearn.feature_extraction.text.HashingVectorizer'> = 0.7777116368610365\n",
      "Размерность данных Train = (1205684, 121903)\n",
      "Размерность данных Test = (157912, 121903)\n",
      " Точность для векторайзера <class 'sklearn.feature_extraction.text.TfidfVectorizer'> = 0.7595242920107401\n",
      "Wall time: 15min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Aнaлизируем по cловам векторайзеры \n",
    "\n",
    "for vectorizer in [CountVectorizer, HashingVectorizer, TfidfVectorizer]:\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "    coder = vectorizer(ngram_range=(1, 5), analyzer='word')\n",
    "\n",
    "    X_train = coder.fit_transform(train_token)\n",
    "    X_test = coder.transform(test_token)\n",
    "    \n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)    \n",
    "    \n",
    "    print(f'Размерность данных Train = {X_train.shape}')\n",
    "    print(f'Размерность данных Test = {X_test.shape}')\n",
    "\n",
    "    logistic_reg = LogisticRegression(random_state=42, max_iter = 100, n_jobs=4)\n",
    "    logistic_reg.fit(X_train, train_enc_labels)\n",
    "    pred = logistic_reg.predict(X_test)\n",
    "    \n",
    "    print(f' Точность для векторайзера {vectorizer} = {accuracy_score(test_enc_labels, pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42a9b1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерность данных Train = (1205684, 1217636)\n",
      "Размерность данных Test = (157912, 1217636)\n",
      " Точность для векторайзеров TfidfVectorizer_char + HashingVectorizer_word = 0.9456089467551547\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Aнaлизируем векторайзер TfidfVectorizer по буквам и HashingVectorizer по словам\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "coder_1 = TfidfVectorizer(ngram_range=(1, 5), analyzer='char')\n",
    "coder_2 = HashingVectorizer(ngram_range=(1, 5), analyzer='word')\n",
    "\n",
    "X_train_1 = coder_1.fit_transform(train_token)\n",
    "X_test_1 = coder_1.transform(test_token)\n",
    "\n",
    "X_train_2 = coder_2.fit_transform(train_token)\n",
    "X_test_2 = coder_2.transform(test_token)\n",
    "\n",
    "\n",
    "X_train = hstack((X_train_1,X_train_2))\n",
    "X_test = hstack((X_test_1,X_test_2))\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)    \n",
    "\n",
    "\n",
    "print(f'Размерность данных Train = {X_train.shape}')\n",
    "print(f'Размерность данных Test = {X_test.shape}')\n",
    "\n",
    "logistic_reg = LogisticRegression(random_state=0, max_iter = 100, n_jobs=4)\n",
    "logistic_reg.fit(X_train, train_enc_labels)\n",
    "\n",
    "pred = logistic_reg.predict(X_test)\n",
    "\n",
    "print(f' Точность для векторайзеров TfidfVectorizer_char и HashingVectorizer_word = {accuracy_score(test_enc_labels, pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54726c41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
